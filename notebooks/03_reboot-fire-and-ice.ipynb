{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Œ Attribution\n",
    "This notebook is a modified and extended version of public work originally shared on Kaggle by [@taylorsamarel](https://www.kaggle.com/taylorsamarel).\n",
    "\n",
    "- ðŸ“˜ [REBOOT: FIRE AND ICE](https://www.kaggle.com/code/taylorsamarel/reboot-fire-and-ice)\n",
    "\n",
    "This notebook is intended solely for educational and research purposes. All original rights remain with the original author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from scipy.stats import pearsonr, rankdata, spearmanr\n",
    "from scipy.optimize import minimize, differential_evolution\n",
    "import gc\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"Starting Bootstrap Stability Analysis for XGBoost Pipeline...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration\n",
    "class CFG:\n",
    "    train_path = \"/kaggle/input/drw-crypto-market-prediction/train.parquet\"\n",
    "    test_path = \"/kaggle/input/drw-crypto-market-prediction/test.parquet\"\n",
    "    sample_sub_path = \"/kaggle/input/drw-crypto-market-prediction/sample_submission.csv\"\n",
    "    \n",
    "    # Bootstrap settings\n",
    "    n_bootstraps = 10  # Number of bootstrap iterations\n",
    "    bootstrap_sample_ratio = 0.8  # Proportion of data to sample\n",
    "    cv_folds = 5  # Cross-validation folds per bootstrap\n",
    "    \n",
    "    # Stability thresholds\n",
    "    feature_stability_threshold = 0.7  # Feature must appear in 70% of bootstraps\n",
    "    score_stability_threshold = 0.05  # Max acceptable std dev in CV scores\n",
    "    \n",
    "    random_state = 42\n",
    "    use_gpu = False\n",
    "    \n",
    "    # Feature settings - balanced approach\n",
    "    max_x_features = 80\n",
    "    n_interaction_features = 50\n",
    "    n_proprietary_features = 30\n",
    "    \n",
    "    # Feature selection\n",
    "    use_feature_selection = True\n",
    "    feature_selection_threshold = 0.01\n",
    "\n",
    "# Bootstrap stability tracker\n",
    "class BootstrapStabilityTracker:\n",
    "    def __init__(self):\n",
    "        self.feature_importance_records = []\n",
    "        self.cv_scores = defaultdict(list)\n",
    "        self.train_scores = defaultdict(list)\n",
    "        self.feature_selection_counts = defaultdict(int)\n",
    "        self.model_predictions = defaultdict(list)\n",
    "        self.bootstrap_indices = []\n",
    "        \n",
    "    def record_iteration(self, iteration, features, importance_df, cv_scores, train_scores, predictions):\n",
    "        \"\"\"Record results from one bootstrap iteration\"\"\"\n",
    "        self.feature_importance_records.append({\n",
    "            'iteration': iteration,\n",
    "            'features': features,\n",
    "            'importance_df': importance_df\n",
    "        })\n",
    "        \n",
    "        for feat in features:\n",
    "            self.feature_selection_counts[feat] += 1\n",
    "            \n",
    "        for model_name, score in cv_scores.items():\n",
    "            self.cv_scores[model_name].append(score)\n",
    "            \n",
    "        for model_name, score in train_scores.items():\n",
    "            self.train_scores[model_name].append(score)\n",
    "            \n",
    "        for model_name, pred in predictions.items():\n",
    "            self.model_predictions[model_name].append(pred)\n",
    "    \n",
    "    def get_stable_features(self, min_appearance_ratio=0.7):\n",
    "        \"\"\"Get features that appear in at least min_appearance_ratio of bootstraps\"\"\"\n",
    "        total_iterations = len(self.feature_importance_records)\n",
    "        stable_features = []\n",
    "        \n",
    "        for feat, count in self.feature_selection_counts.items():\n",
    "            if count / total_iterations >= min_appearance_ratio:\n",
    "                stable_features.append((feat, count / total_iterations))\n",
    "                \n",
    "        return sorted(stable_features, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    def get_model_stability_metrics(self):\n",
    "        \"\"\"Calculate stability metrics for each model\"\"\"\n",
    "        stability_metrics = {}\n",
    "        \n",
    "        for model_name in self.cv_scores:\n",
    "            cv_scores_array = np.array(self.cv_scores[model_name])\n",
    "            train_scores_array = np.array(self.train_scores[model_name])\n",
    "            \n",
    "            stability_metrics[model_name] = {\n",
    "                'cv_mean': np.mean(cv_scores_array),\n",
    "                'cv_std': np.std(cv_scores_array),\n",
    "                'cv_min': np.min(cv_scores_array),\n",
    "                'cv_max': np.max(cv_scores_array),\n",
    "                'train_mean': np.mean(train_scores_array),\n",
    "                'train_std': np.std(train_scores_array),\n",
    "                'overfit_score': np.mean(train_scores_array) - np.mean(cv_scores_array)\n",
    "            }\n",
    "            \n",
    "        return stability_metrics\n",
    "    \n",
    "    def plot_stability_analysis(self):\n",
    "        \"\"\"Create visualization of stability analysis\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Plot 1: Feature appearance frequency\n",
    "        stable_features = self.get_stable_features(0.5)[:20]\n",
    "        features, frequencies = zip(*stable_features)\n",
    "        \n",
    "        axes[0, 0].barh(features, frequencies)\n",
    "        axes[0, 0].set_xlabel('Appearance Frequency')\n",
    "        axes[0, 0].set_title('Top 20 Most Stable Features')\n",
    "        axes[0, 0].axvline(x=0.7, color='r', linestyle='--', label='Stability Threshold')\n",
    "        \n",
    "        # Plot 2: Model CV score distributions\n",
    "        model_names = list(self.cv_scores.keys())\n",
    "        cv_data = [self.cv_scores[name] for name in model_names]\n",
    "        \n",
    "        axes[0, 1].boxplot(cv_data, labels=model_names)\n",
    "        axes[0, 1].set_ylabel('CV Score')\n",
    "        axes[0, 1].set_title('Model CV Score Distributions')\n",
    "        axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        # Plot 3: Train vs CV scores\n",
    "        stability_metrics = self.get_model_stability_metrics()\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            metrics = stability_metrics[model_name]\n",
    "            axes[1, 0].scatter(metrics['cv_mean'], metrics['train_mean'], \n",
    "                             s=100, label=model_name, alpha=0.7)\n",
    "            axes[1, 0].errorbar(metrics['cv_mean'], metrics['train_mean'],\n",
    "                              xerr=metrics['cv_std'], yerr=metrics['train_std'],\n",
    "                              alpha=0.3)\n",
    "        \n",
    "        # Add diagonal line\n",
    "        min_val = min(axes[1, 0].get_xlim()[0], axes[1, 0].get_ylim()[0])\n",
    "        max_val = max(axes[1, 0].get_xlim()[1], axes[1, 0].get_ylim()[1])\n",
    "        axes[1, 0].plot([min_val, max_val], [min_val, max_val], 'k--', alpha=0.3)\n",
    "        \n",
    "        axes[1, 0].set_xlabel('Mean CV Score')\n",
    "        axes[1, 0].set_ylabel('Mean Train Score')\n",
    "        axes[1, 0].set_title('Train vs CV Performance')\n",
    "        axes[1, 0].legend()\n",
    "        \n",
    "        # Plot 4: Overfitting scores\n",
    "        overfit_scores = [(name, metrics['overfit_score']) \n",
    "                         for name, metrics in stability_metrics.items()]\n",
    "        overfit_scores.sort(key=lambda x: x[1])\n",
    "        \n",
    "        names, scores = zip(*overfit_scores)\n",
    "        colors = ['green' if s < 0.1 else 'orange' if s < 0.2 else 'red' for s in scores]\n",
    "        \n",
    "        axes[1, 1].barh(names, scores, color=colors)\n",
    "        axes[1, 1].set_xlabel('Overfitting Score (Train - CV)')\n",
    "        axes[1, 1].set_title('Model Overfitting Analysis')\n",
    "        axes[1, 1].axvline(x=0.1, color='g', linestyle='--', alpha=0.5)\n",
    "        axes[1, 1].axvline(x=0.2, color='r', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('bootstrap_stability_analysis.png', dpi=300)\n",
    "        plt.show()\n",
    "\n",
    "# Memory optimization (keep from original)\n",
    "def reduce_mem_usage(df, name=\"\"):\n",
    "    print(f\"Optimizing memory for {name}...\")\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(f'Memory usage: {start_mem:.2f} MB -> {end_mem:.2f} MB ({100*(start_mem-end_mem)/start_mem:.1f}% reduction)')\n",
    "    return df\n",
    "\n",
    "# Create bootstrap sample\n",
    "def create_bootstrap_sample(df, sample_ratio=0.8, random_state=None):\n",
    "    \"\"\"Create a bootstrap sample of the dataframe\"\"\"\n",
    "    n_samples = int(len(df) * sample_ratio)\n",
    "    indices = np.random.RandomState(random_state).choice(len(df), n_samples, replace=True)\n",
    "    return df.iloc[indices].reset_index(drop=True), indices\n",
    "\n",
    "# Cross-validation with stability tracking\n",
    "def stable_cross_validation(X, y, model_params, n_folds=5, random_state=42):\n",
    "    \"\"\"Perform cross-validation and return both mean score and fold scores\"\"\"\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=random_state)\n",
    "    fold_scores = []\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "        X_fold_train = X.iloc[train_idx] if hasattr(X, 'iloc') else X[train_idx]\n",
    "        y_fold_train = y.iloc[train_idx] if hasattr(y, 'iloc') else y[train_idx]\n",
    "        X_fold_valid = X.iloc[valid_idx] if hasattr(X, 'iloc') else X[valid_idx]\n",
    "        y_fold_valid = y.iloc[valid_idx] if hasattr(y, 'iloc') else y[valid_idx]\n",
    "        \n",
    "        model = XGBRegressor(**model_params)\n",
    "        model.fit(X_fold_train, y_fold_train)\n",
    "        \n",
    "        pred_valid = model.predict(X_fold_valid)\n",
    "        fold_score = pearsonr(y_fold_valid, pred_valid)[0]\n",
    "        fold_scores.append(fold_score)\n",
    "    \n",
    "    return np.mean(fold_scores), np.std(fold_scores), fold_scores\n",
    "\n",
    "# Feature importance with stability\n",
    "def get_stable_feature_importance(X, y, feature_names, n_runs=5):\n",
    "    \"\"\"Get feature importance with stability across multiple runs\"\"\"\n",
    "    importance_runs = []\n",
    "    \n",
    "    for i in range(n_runs):\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'random_state': 42 + i,\n",
    "            'n_jobs': -1,\n",
    "            'verbosity': 0\n",
    "        }\n",
    "        \n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X, y)\n",
    "        importance_runs.append(model.feature_importances_)\n",
    "    \n",
    "    # Calculate mean and std of importance\n",
    "    importance_mean = np.mean(importance_runs, axis=0)\n",
    "    importance_std = np.std(importance_runs, axis=0)\n",
    "    \n",
    "    # Create DataFrame with stability metrics\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance_mean': importance_mean,\n",
    "        'importance_std': importance_std,\n",
    "        'importance_cv': importance_std / (importance_mean + 1e-8),\n",
    "        'stability_score': importance_mean / (importance_std + 1e-8)\n",
    "    }).sort_values('importance_mean', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Modified feature engineering functions (keep from original but add stability tracking)\n",
    "def create_proprietary_x_variables(df, n_features=30):\n",
    "    \"\"\"Create proprietary X variables with focus on quality over quantity\"\"\"\n",
    "    print(f\"Creating {n_features} proprietary X variables...\")\n",
    "    \n",
    "    # Get existing X features\n",
    "    x_features = [col for col in df.columns if col.startswith('X') and col[1:].isdigit()]\n",
    "    \n",
    "    # Important X features from original pipeline\n",
    "    important_x = [\"X752\", \"X287\", \"X298\", \"X759\", \"X302\", \"X55\", \"X56\", \"X52\", \"X303\", \"X51\",\n",
    "                   \"X344\", \"X598\", \"X385\", \"X603\", \"X674\", \"X415\", \"X345\", \"X137\", \"X174\", \"X178\"]\n",
    "    \n",
    "    # Use available important features\n",
    "    base_features = [f for f in important_x if f in df.columns][:10]\n",
    "    \n",
    "    # Add some high-variance features\n",
    "    if len(base_features) < 10:\n",
    "        x_variances = df[x_features].var()\n",
    "        high_var_features = x_variances.nlargest(10).index.tolist()\n",
    "        for feat in high_var_features:\n",
    "            if feat not in base_features:\n",
    "                base_features.append(feat)\n",
    "                if len(base_features) >= 10:\n",
    "                    break\n",
    "    \n",
    "    prop_idx = 1\n",
    "    \n",
    "    # 1. Statistical combinations (8 features)\n",
    "    if len(base_features) >= 5:\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:5]].mean(axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:5]].std(axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:5]].max(axis=1) - df[base_features[:5]].min(axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:5]].median(axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:7]].quantile(0.25, axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:7]].quantile(0.75, axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = (df[base_features[:5]] > df[base_features[:5]].mean(axis=1).values[:, None]).sum(axis=1)\n",
    "        prop_idx += 1\n",
    "        df[f'X_prop_{prop_idx}'] = df[base_features[:5]].idxmax(axis=1).str.extract('(\\d+)')[0].astype(float)\n",
    "        prop_idx += 1\n",
    "    \n",
    "    # 2. Non-linear transformations (7 features)\n",
    "    for i in range(7):\n",
    "        feat = base_features[i % len(base_features)]\n",
    "        if i < 2:\n",
    "            df[f'X_prop_{prop_idx}'] = np.sign(df[feat]) * np.sqrt(np.abs(df[feat]))\n",
    "        elif i < 4:\n",
    "            df[f'X_prop_{prop_idx}'] = np.tanh(df[feat] / df[feat].std())\n",
    "        elif i < 6:\n",
    "            df[f'X_prop_{prop_idx}'] = 1 / (1 + np.exp(-df[feat] / df[feat].std()))  # Sigmoid\n",
    "        else:\n",
    "            df[f'X_prop_{prop_idx}'] = rankdata(df[feat]) / len(df)  # Rank transform\n",
    "        prop_idx += 1\n",
    "    \n",
    "    # 3. Market interaction features (10 features)\n",
    "    if 'volume' in df.columns:\n",
    "        for i in range(3):\n",
    "            df[f'X_prop_{prop_idx}'] = df[base_features[i]] * np.log1p(df['volume'])\n",
    "            prop_idx += 1\n",
    "    \n",
    "    if 'order_flow_imbalance' in df.columns:\n",
    "        for i in range(3):\n",
    "            df[f'X_prop_{prop_idx}'] = df[base_features[i+3]] * df['order_flow_imbalance']\n",
    "            prop_idx += 1\n",
    "    \n",
    "    if 'kyle_lambda' in df.columns:\n",
    "        for i in range(2):\n",
    "            df[f'X_prop_{prop_idx}'] = df[base_features[i+6]] * np.sign(df['kyle_lambda']) * np.log1p(np.abs(df['kyle_lambda']))\n",
    "            prop_idx += 1\n",
    "    \n",
    "    if 'vpin' in df.columns:\n",
    "        for i in range(2):\n",
    "            df[f'X_prop_{prop_idx}'] = df[base_features[i+8]] * df['vpin']\n",
    "            prop_idx += 1\n",
    "    \n",
    "    # 4. Interaction ratios (5 features)\n",
    "    for i in range(5):\n",
    "        feat1 = base_features[i % len(base_features)]\n",
    "        feat2 = base_features[(i + 1) % len(base_features)]\n",
    "        df[f'X_prop_{prop_idx}'] = df[feat1] / (np.abs(df[feat2]) + 1e-8)\n",
    "        prop_idx += 1\n",
    "    \n",
    "    # Handle infinities and NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    print(f\"Created {prop_idx-1} proprietary X variables\")\n",
    "    return df\n",
    "\n",
    "def create_interaction_features(df, selected_features, n_interactions=50):\n",
    "    \"\"\"Create high-quality interaction features\"\"\"\n",
    "    print(f\"Creating {n_interactions} interaction features...\")\n",
    "    \n",
    "    interaction_features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Prioritize features\n",
    "    important_x = [\"X752\", \"X287\", \"X298\", \"X759\", \"X302\", \"X55\", \"X56\", \"X52\", \"X303\", \"X51\"]\n",
    "    market_features = ['order_flow_imbalance', 'kyle_lambda', 'vpin', 'liquidity_imbalance',\n",
    "                      'bid_ask_spread', 'buying_pressure', 'volume', 'log_volume']\n",
    "    \n",
    "    # Get available priority features\n",
    "    priority_x = [f for f in important_x if f in selected_features and f in df.columns][:10]\n",
    "    priority_market = [f for f in market_features if f in selected_features and f in df.columns][:8]\n",
    "    \n",
    "    interaction_count = 0\n",
    "    \n",
    "    # 1. X features with market microstructure (20 interactions)\n",
    "    for i, x_feat in enumerate(priority_x[:10]):\n",
    "        if interaction_count >= 20:\n",
    "            break\n",
    "        for j, market_feat in enumerate(priority_market[:4]):\n",
    "            if interaction_count >= 20:\n",
    "                break\n",
    "            \n",
    "            # Multiplication\n",
    "            interaction_features.append(df[x_feat] * df[market_feat])\n",
    "            feature_names.append(f'{x_feat}_x_{market_feat}')\n",
    "            interaction_count += 1\n",
    "            \n",
    "            # Log interaction\n",
    "            if interaction_count < 20 and market_feat in ['volume', 'kyle_lambda']:\n",
    "                interaction_features.append(df[x_feat] * np.log1p(np.abs(df[market_feat])))\n",
    "                feature_names.append(f'{x_feat}_x_log_{market_feat}')\n",
    "                interaction_count += 1\n",
    "    \n",
    "    # 2. Market feature interactions (15 interactions)\n",
    "    market_pairs = [\n",
    "        ('order_flow_imbalance', 'kyle_lambda'),\n",
    "        ('vpin', 'liquidity_imbalance'),\n",
    "        ('buying_pressure', 'selling_pressure'),\n",
    "        ('bid_ask_spread', 'total_liquidity'),\n",
    "        ('volume', 'liquidity_ratio')\n",
    "    ]\n",
    "    \n",
    "    for feat1, feat2 in market_pairs:\n",
    "        if interaction_count >= 35:\n",
    "            break\n",
    "        if feat1 in df.columns and feat2 in df.columns:\n",
    "            # Product\n",
    "            interaction_features.append(df[feat1] * df[feat2])\n",
    "            feature_names.append(f'{feat1}_x_{feat2}')\n",
    "            interaction_count += 1\n",
    "            \n",
    "            # Ratio\n",
    "            if interaction_count < 35:\n",
    "                interaction_features.append(df[feat1] / (np.abs(df[feat2]) + 1e-8))\n",
    "                feature_names.append(f'{feat1}_div_{feat2}')\n",
    "                interaction_count += 1\n",
    "            \n",
    "            # Difference\n",
    "            if interaction_count < 35:\n",
    "                interaction_features.append(df[feat1] - df[feat2])\n",
    "                feature_names.append(f'{feat1}_minus_{feat2}')\n",
    "                interaction_count += 1\n",
    "    \n",
    "    # 3. Non-linear interactions (10 interactions)\n",
    "    for i in range(5):\n",
    "        if interaction_count >= 45:\n",
    "            break\n",
    "        if i < len(priority_x):\n",
    "            feat = priority_x[i]\n",
    "            \n",
    "            # Squared\n",
    "            interaction_features.append(df[feat] ** 2)\n",
    "            feature_names.append(f'{feat}_squared')\n",
    "            interaction_count += 1\n",
    "            \n",
    "            # Square root of absolute\n",
    "            if interaction_count < 45:\n",
    "                interaction_features.append(np.sqrt(np.abs(df[feat])))\n",
    "                feature_names.append(f'{feat}_sqrt_abs')\n",
    "                interaction_count += 1\n",
    "    \n",
    "    # 4. Three-way interactions (5 interactions)\n",
    "    if len(priority_x) >= 3 and len(priority_market) >= 1:\n",
    "        for i in range(5):\n",
    "            if interaction_count >= n_interactions:\n",
    "                break\n",
    "            x1 = priority_x[i % len(priority_x)]\n",
    "            x2 = priority_x[(i+1) % len(priority_x)]\n",
    "            m1 = priority_market[i % len(priority_market)]\n",
    "            \n",
    "            interaction_features.append(df[x1] * df[x2] * df[m1])\n",
    "            feature_names.append(f'{x1}_{x2}_{m1}')\n",
    "            interaction_count += 1\n",
    "    \n",
    "    # Create DataFrame\n",
    "    interaction_df = pd.DataFrame(\n",
    "        np.column_stack(interaction_features[:interaction_count]),\n",
    "        columns=feature_names[:interaction_count],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # Handle infinities and NaN\n",
    "    interaction_df = interaction_df.replace([np.inf, -np.inf], np.nan)\n",
    "    interaction_df = interaction_df.fillna(0)\n",
    "    \n",
    "    print(f\"Created {interaction_count} interaction features\")\n",
    "    return interaction_df\n",
    "\n",
    "def add_features(df):\n",
    "    \"\"\"Create comprehensive features for market microstructure\"\"\"\n",
    "    print(\"Engineering features...\")\n",
    "    \n",
    "    # Basic interactions (from original)\n",
    "    df['bid_ask_spread'] = df['ask_qty'] - df['bid_qty']\n",
    "    df['bid_ask_ratio'] = df['bid_qty'] / (df['ask_qty'] + 1e-8)\n",
    "    df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-8)\n",
    "    df['order_flow_imbalance'] = (df['buy_qty'] - df['sell_qty']) / (df['volume'] + 1e-8)\n",
    "    \n",
    "    # Pressure indicators\n",
    "    df['buying_pressure'] = df['buy_qty'] / (df['volume'] + 1e-8)\n",
    "    df['selling_pressure'] = df['sell_qty'] / (df['volume'] + 1e-8)\n",
    "    df['net_pressure'] = df['buying_pressure'] - df['selling_pressure']\n",
    "    \n",
    "    # Liquidity features\n",
    "    df['total_liquidity'] = df['bid_qty'] + df['ask_qty']\n",
    "    df['liquidity_imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['total_liquidity'] + 1e-8)\n",
    "    df['liquidity_ratio'] = df['total_liquidity'] / (df['volume'] + 1e-8)\n",
    "    \n",
    "    # Volume transformations\n",
    "    df['log_volume'] = np.log1p(df['volume'])\n",
    "    df['sqrt_volume'] = np.sqrt(df['volume'])\n",
    "    \n",
    "    # Market microstructure\n",
    "    df['kyle_lambda'] = df['order_flow_imbalance'] / (df['sqrt_volume'] + 1e-8)\n",
    "    df['vpin'] = np.abs(df['buy_qty'] - df['sell_qty']) / (df['buy_qty'] + df['sell_qty'] + 1e-8)\n",
    "    \n",
    "    # Additional useful features from extended version\n",
    "    df['effective_spread'] = 2 * np.abs(df['order_flow_imbalance']) * df['bid_ask_spread']\n",
    "    df['realized_spread'] = df['bid_ask_spread'] * df['vpin']\n",
    "    df['price_impact'] = df['kyle_lambda'] * df['volume']\n",
    "    df['trade_intensity'] = df['volume'] / (df['total_liquidity'] + 1e-8)\n",
    "    \n",
    "    # Handle infinities and NaN\n",
    "    df = df.replace([np.inf, -np.inf], np.nan)\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Main bootstrap pipeline\n",
    "def run_bootstrap_iteration(train, test, iteration, tracker, cfg):\n",
    "    \"\"\"Run one bootstrap iteration\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Bootstrap Iteration {iteration + 1}/{cfg.n_bootstraps}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Create bootstrap sample\n",
    "    train_boot, boot_indices = create_bootstrap_sample(\n",
    "        train, \n",
    "        sample_ratio=cfg.bootstrap_sample_ratio,\n",
    "        random_state=cfg.random_state + iteration\n",
    "    )\n",
    "    tracker.bootstrap_indices.append(boot_indices)\n",
    "    \n",
    "    # Create proprietary features\n",
    "    train_boot = create_proprietary_x_variables(train_boot, cfg.n_proprietary_features)\n",
    "    test_boot = test.copy()\n",
    "    test_boot = create_proprietary_x_variables(test_boot, cfg.n_proprietary_features)\n",
    "    \n",
    "    # Feature engineering\n",
    "    train_boot = add_features(train_boot)\n",
    "    test_boot = add_features(test_boot)\n",
    "    \n",
    "    # Select base features\n",
    "    selected_x_features = [\n",
    "        \"X752\", \"X287\", \"X298\", \"X759\", \"X302\", \"X55\", \"X56\", \"X52\", \"X303\", \"X51\",\n",
    "        \"X344\", \"X598\", \"X385\", \"X603\", \"X674\", \"X415\", \"X345\", \"X137\", \"X174\", \"X178\"\n",
    "    ]\n",
    "    \n",
    "    # Add proprietary features\n",
    "    proprietary_features = [f\"X_prop_{i}\" for i in range(1, cfg.n_proprietary_features + 1)]\n",
    "    selected_x_features.extend(proprietary_features)\n",
    "    \n",
    "    # Get all X features and add more if needed\n",
    "    all_x_features = [col for col in train_boot.columns if col.startswith('X') and col[1:].isdigit()]\n",
    "    additional_x = [f for f in all_x_features if f not in selected_x_features][:cfg.max_x_features - len(selected_x_features)]\n",
    "    selected_x_features.extend(additional_x)\n",
    "    \n",
    "    available_x_features = [f for f in selected_x_features if f in train_boot.columns]\n",
    "    \n",
    "    # Market and engineered features\n",
    "    market_features = ['bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n",
    "    engineered_features = [col for col in train_boot.columns if col in [\n",
    "        'bid_ask_spread', 'bid_ask_ratio', 'buy_sell_ratio', 'order_flow_imbalance',\n",
    "        'buying_pressure', 'selling_pressure', 'net_pressure', 'total_liquidity', \n",
    "        'liquidity_imbalance', 'liquidity_ratio', 'log_volume', 'sqrt_volume',\n",
    "        'kyle_lambda', 'vpin', 'effective_spread', 'realized_spread', \n",
    "        'price_impact', 'trade_intensity'\n",
    "    ]]\n",
    "    \n",
    "    # Combine base features\n",
    "    base_selected_features = market_features + available_x_features + engineered_features\n",
    "    base_selected_features = list(dict.fromkeys(base_selected_features))\n",
    "    base_selected_features = [f for f in base_selected_features if f in train_boot.columns]\n",
    "    \n",
    "    # Create interaction features\n",
    "    interaction_df_train = create_interaction_features(train_boot, base_selected_features, cfg.n_interaction_features)\n",
    "    interaction_df_test = create_interaction_features(test_boot, base_selected_features, cfg.n_interaction_features)\n",
    "    \n",
    "    # Add interaction features\n",
    "    for col in interaction_df_train.columns:\n",
    "        train_boot[col] = interaction_df_train[col]\n",
    "        test_boot[col] = interaction_df_test[col]\n",
    "    \n",
    "    # All features before selection\n",
    "    all_features = base_selected_features + list(interaction_df_train.columns)\n",
    "    \n",
    "    # Prepare data for feature selection\n",
    "    X_train_all = train_boot[all_features]\n",
    "    y_train = train_boot['label']\n",
    "    X_test_all = test_boot[all_features]\n",
    "    \n",
    "    # Feature selection with stability\n",
    "    if cfg.use_feature_selection:\n",
    "        importance_df = get_stable_feature_importance(\n",
    "            X_train_all, y_train, all_features, n_runs=3\n",
    "        )\n",
    "        \n",
    "        # Select features based on mean importance and stability\n",
    "        selected_features = importance_df[\n",
    "            (importance_df['importance_mean'] > cfg.feature_selection_threshold) &\n",
    "            (importance_df['importance_cv'] < 0.5)  # Coefficient of variation < 0.5\n",
    "        ]['feature'].tolist()\n",
    "        \n",
    "        # Always include critical features\n",
    "        critical_features = ['order_flow_imbalance', 'kyle_lambda', 'vpin', 'volume', \n",
    "                            'bid_ask_spread', 'liquidity_imbalance', 'buying_pressure']\n",
    "        \n",
    "        for feat in critical_features:\n",
    "            if feat in all_features and feat not in selected_features:\n",
    "                selected_features.append(feat)\n",
    "    else:\n",
    "        selected_features = all_features\n",
    "        importance_df = pd.DataFrame()\n",
    "    \n",
    "    X_train = X_train_all[selected_features]\n",
    "    X_test = X_test_all[selected_features]\n",
    "    \n",
    "    print(f\"Selected {len(selected_features)} features for this iteration\")\n",
    "    \n",
    "    # Initialize storage for this iteration\n",
    "    cv_scores = {}\n",
    "    train_scores = {}\n",
    "    predictions = {}\n",
    "    \n",
    "    # Model configurations\n",
    "    model_configs = [\n",
    "        {\n",
    "            'name': 'conservative',\n",
    "            'params': {\n",
    "                'n_estimators': 800,\n",
    "                'max_depth': 5,\n",
    "                'learning_rate': 0.012,\n",
    "                'subsample': 0.65,\n",
    "                'colsample_bytree': 0.65,\n",
    "                'reg_alpha': 2.5,\n",
    "                'reg_lambda': 2.5,\n",
    "                'min_child_weight': 25,\n",
    "                'gamma': 0.3,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'balanced',\n",
    "            'params': {\n",
    "                'n_estimators': 600,\n",
    "                'max_depth': 7,\n",
    "                'learning_rate': 0.018,\n",
    "                'subsample': 0.75,\n",
    "                'colsample_bytree': 0.75,\n",
    "                'reg_alpha': 1.0,\n",
    "                'reg_lambda': 1.0,\n",
    "                'min_child_weight': 10,\n",
    "                'gamma': 0.1,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            'name': 'aggressive',\n",
    "            'params': {\n",
    "                'n_estimators': 500,\n",
    "                'max_depth': 9,\n",
    "                'learning_rate': 0.025,\n",
    "                'subsample': 0.85,\n",
    "                'colsample_bytree': 0.85,\n",
    "                'reg_alpha': 0.3,\n",
    "                'reg_lambda': 0.3,\n",
    "                'min_child_weight': 5,\n",
    "                'gamma': 0.05,\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1,\n",
    "                'verbosity': 0\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Standardize data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = pd.DataFrame(\n",
    "        scaler.fit_transform(X_train),\n",
    "        columns=selected_features,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    X_test_scaled = pd.DataFrame(\n",
    "        scaler.transform(X_test),\n",
    "        columns=selected_features,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    # Train and evaluate each model configuration\n",
    "    for config in model_configs:\n",
    "        print(f\"\\nEvaluating {config['name']} model...\")\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_mean, cv_std, fold_scores = stable_cross_validation(\n",
    "            X_train_scaled, y_train, config['params'], \n",
    "            n_folds=cfg.cv_folds, \n",
    "            random_state=cfg.random_state + iteration\n",
    "        )\n",
    "        \n",
    "        # Full training\n",
    "        model = XGBRegressor(**config['params'])\n",
    "        model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        train_pred = model.predict(X_train_scaled)\n",
    "        test_pred = model.predict(X_test_scaled)\n",
    "        \n",
    "        train_score = pearsonr(y_train, train_pred)[0]\n",
    "        \n",
    "        # Store results\n",
    "        cv_scores[config['name']] = cv_mean\n",
    "        train_scores[config['name']] = train_score\n",
    "        predictions[config['name']] = test_pred\n",
    "        \n",
    "        print(f\"  CV Score: {cv_mean:.4f} (Â±{cv_std:.4f})\")\n",
    "        print(f\"  Train Score: {train_score:.4f}\")\n",
    "        print(f\"  Overfitting: {train_score - cv_mean:.4f}\")\n",
    "    \n",
    "    # Record iteration results\n",
    "    tracker.record_iteration(\n",
    "        iteration, selected_features, importance_df,\n",
    "        cv_scores, train_scores, predictions\n",
    "    )\n",
    "    \n",
    "    return selected_features, cv_scores, train_scores\n",
    "\n",
    "# Main execution\n",
    "print(\"\\nLoading data...\")\n",
    "train = pd.read_parquet(CFG.train_path)\n",
    "test = pd.read_parquet(CFG.test_path)\n",
    "submission = pd.read_csv(CFG.sample_sub_path)\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "\n",
    "# Memory optimization\n",
    "train = reduce_mem_usage(train, \"train\")\n",
    "test = reduce_mem_usage(test, \"test\")\n",
    "\n",
    "# Initialize tracker\n",
    "tracker = BootstrapStabilityTracker()\n",
    "\n",
    "# Run bootstrap iterations\n",
    "for i in range(CFG.n_bootstraps):\n",
    "    selected_features, cv_scores, train_scores = run_bootstrap_iteration(\n",
    "        train, test, i, tracker, CFG\n",
    "    )\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Bootstrap Stability Analysis Results\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get stable features\n",
    "stable_features = tracker.get_stable_features(CFG.feature_stability_threshold)\n",
    "print(f\"\\nFound {len(stable_features)} stable features (appearing in â‰¥{CFG.feature_stability_threshold*100}% of bootstraps)\")\n",
    "print(\"\\nTop 20 most stable features:\")\n",
    "for feat, freq in stable_features[:20]:\n",
    "    print(f\"  {feat}: {freq*100:.1f}%\")\n",
    "\n",
    "# Get model stability metrics\n",
    "stability_metrics = tracker.get_model_stability_metrics()\n",
    "print(\"\\nModel Stability Metrics:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Model':<15} {'CV Mean':<10} {'CV Std':<10} {'Train Mean':<10} {'Overfit':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for model_name, metrics in stability_metrics.items():\n",
    "    print(f\"{model_name:<15} {metrics['cv_mean']:<10.4f} {metrics['cv_std']:<10.4f} \"\n",
    "          f\"{metrics['train_mean']:<10.4f} {metrics['overfit_score']:<10.4f}\")\n",
    "\n",
    "# Find most stable model\n",
    "most_stable_model = min(stability_metrics.items(), \n",
    "                       key=lambda x: x[1]['cv_std'] + abs(x[1]['overfit_score']))\n",
    "print(f\"\\nMost stable model: {most_stable_model[0]}\")\n",
    "\n",
    "# Create final ensemble using stable features only\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Creating Final Stable Ensemble\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use only features that appear in most bootstraps\n",
    "final_features = [feat for feat, freq in stable_features if freq >= CFG.feature_stability_threshold]\n",
    "\n",
    "# Prepare final dataset with stable features\n",
    "# (Re-engineer features on full dataset)\n",
    "train_final = train.copy()\n",
    "test_final = test.copy()\n",
    "\n",
    "train_final = create_proprietary_x_variables(train_final, CFG.n_proprietary_features)\n",
    "test_final = create_proprietary_x_variables(test_final, CFG.n_proprietary_features)\n",
    "\n",
    "train_final = add_features(train_final)\n",
    "test_final = add_features(test_final)\n",
    "\n",
    "# Get available stable features\n",
    "available_stable_features = [f for f in final_features if f in train_final.columns]\n",
    "\n",
    "# Create final predictions using ensemble of stable models\n",
    "final_predictions = []\n",
    "\n",
    "for model_name in tracker.model_predictions:\n",
    "    if stability_metrics[model_name]['cv_std'] < CFG.score_stability_threshold:\n",
    "        # Average predictions from all bootstrap iterations for stable models\n",
    "        model_preds = np.mean(tracker.model_predictions[model_name], axis=0)\n",
    "        final_predictions.append(model_preds)\n",
    "        print(f\"Including {model_name} in final ensemble\")\n",
    "\n",
    "# Create final ensemble prediction\n",
    "if final_predictions:\n",
    "    final_pred = np.mean(final_predictions, axis=0)\n",
    "else:\n",
    "    print(\"Warning: No stable models found, using all models\")\n",
    "    final_pred = np.mean([np.mean(preds, axis=0) \n",
    "                         for preds in tracker.model_predictions.values()], axis=0)\n",
    "\n",
    "# Post-processing\n",
    "y_train = train['label']\n",
    "p1, p99 = np.percentile(y_train, [1, 99])\n",
    "final_pred = np.clip(final_pred, p1, p99)\n",
    "\n",
    "# Create submission\n",
    "submission['prediction'] = final_pred\n",
    "submission.to_csv('stable_submission.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Stable submission saved to stable_submission.csv\")\n",
    "print(submission.head())\n",
    "\n",
    "# Create stability report\n",
    "stability_report = pd.DataFrame({\n",
    "    'feature': [f[0] for f in stable_features[:50]],\n",
    "    'stability_score': [f[1] for f in stable_features[:50]]\n",
    "})\n",
    "stability_report.to_csv('feature_stability_report.csv', index=False)\n",
    "\n",
    "model_stability_df = pd.DataFrame(stability_metrics).T\n",
    "model_stability_df.to_csv('model_stability_report.csv')\n",
    "\n",
    "# Plot stability analysis\n",
    "tracker.plot_stability_analysis()\n",
    "\n",
    "print(\"\\nStability analysis complete!\")\n",
    "print(\"Files generated:\")\n",
    "print(\"  - stable_submission.csv: Final predictions using stable features/models\")\n",
    "print(\"  - feature_stability_report.csv: Feature stability analysis\")\n",
    "print(\"  - model_stability_report.csv: Model stability metrics\")\n",
    "print(\"  - bootstrap_stability_analysis.png: Visualization of stability analysis\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Bootstrap Stability Pipeline Completed Successfully!\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 12993472,
     "sourceId": 96164,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
